{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling2D, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "import wave\n",
    "import struct\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import python_speech_features as ps\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import pyaudio\n",
    "import math\n",
    "import datetime\n",
    "from concurrent.futures.process import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import time\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotionAnalizer():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alive = True\n",
    "        self.extractMellalive = False\n",
    "        #inputAudioに必要な初期化\n",
    "        self.CHUNK=1024\n",
    "        self.RATE = 44100\n",
    "        self.wave = np.empty((0,1024),int)\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.stream=self.audio.open(format = pyaudio.paInt16,\n",
    "            channels = 1,\n",
    "            rate = self.RATE,\n",
    "            frames_per_buffer = self.CHUNK,\n",
    "            input = True,\n",
    "            output=True)\n",
    "        #detectAudioに必要な初期化\n",
    "        self.spaceleng = 10\n",
    "        self.voiceleng = 10\n",
    "        self.sokuonleng = 3\n",
    "        self.winlen = 0.08\n",
    "        self.winstep = 0.016\n",
    "        self.nfilt = 40 #周波数の分解能\n",
    "        self.waddr = 0\n",
    "        self.ave = np.empty((1,), float) #diffで過去のデータを参照するなので1つ余分に作っておく。\n",
    "        self.diff = np.empty((0,), float)\n",
    "        self.silent = 0\n",
    "        self.voice  = 0\n",
    "        self.standby = -1\n",
    "        self.isVoice = 0\n",
    "        self.voiceOnWave = np.empty((0,2,2), int)\n",
    "        #extractMellに必要な初期化\n",
    "        self.pastVoiceOnWaveLeng = 0\n",
    "        self.lastwaddr = 0\n",
    "        self.features = np.zeros((0, 300, self.nfilt, 3))\n",
    "        self.eps = 1e-5\n",
    "        with open('mean_and_std.pkl', 'rb') as file:\n",
    "          self.mean1,self.std1,self.mean2,self.std2,self.mean3,self.std3 = pickle.load(file)\n",
    "        #emotionRecognitionに必要な初期化\n",
    "        self.faddr = 0\n",
    "        self.emotionResult = np.zeros((0,14))\n",
    "        # loading json and model architecture \n",
    "        json_file = open('model_json.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into new model\n",
    "        self.loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "\n",
    "    def inputAudio(self):\n",
    "            #print(\"inputAudio Start\")\n",
    "            while self.alive:\n",
    "                ret = self.stream.read(self.CHUNK, exception_on_overflow = False)\n",
    "                self.stream.write(ret)\n",
    "                self.wave = np.append(self.wave, np.expand_dims(np.frombuffer(ret, dtype=\"int16\"), 0), axis=0)\n",
    "            #print(\"inputAudio End\")\n",
    "        \n",
    "    def detectVoice(self):\n",
    "        #print(\"detectVoice Start\")\n",
    "        #time.sleep(0.001)\n",
    "        if self.wave.shape[0] > 0:\n",
    "            self.waddr = self.wave.shape[0]-1 #wave変数の最新のデータは、その時のwaveの要素数から1引いたもの\n",
    "            self.ave = np.append(self.ave, np.mean(np.abs(self.wave[self.waddr])))\n",
    "            self.diff = np.append(self.diff, self.ave[-2]-self.ave[-1])\n",
    "            #声の部分か判定する。\n",
    "            if np.abs(self.diff[self.waddr]) > 100:\n",
    "                self.silent = 0\n",
    "                self.voice  = self.sokuonleng\n",
    "            else:\n",
    "                self.silent += 1\n",
    "                self.voice  -= 1 if self.voice > 0 else 0\n",
    "            #発話していないなら\n",
    "            if self.isVoice == 0:\n",
    "                if self.standby == -1:\n",
    "                    #スタンバイでない時、音を検知したら、スタンバイする(その時の時間を覚えておく)。\n",
    "                    if self.voice == self.sokuonleng:\n",
    "                        self.standby = self.waddr - 1 #発声する瞬間も大事な特微量なので、現在のアドレスより1コマ過去のものから判定開始\n",
    "                else:\n",
    "                    if self.waddr-self.standby >= self.voiceleng:\n",
    "                        #voiceleng以上発話していれば、isRecをアクティブに。\n",
    "                        if self.voice != 0:\n",
    "                            print(\"Record start{}\".format(self.standby, self.waddr))\n",
    "                            self.isVoice = 1\n",
    "                            self.voiceOnWave = np.append(self.voiceOnWave, np.zeros((1,2,2), int),0)\n",
    "                            self.voiceOnWave[-1,0,0] = self.standby\n",
    "                        #voicelengより短い発話だったら、スタンバイ解除\n",
    "                        elif self.voice == 0:\n",
    "                            self.standby = -1\n",
    "            #発話しているなら\n",
    "            else:\n",
    "                #もしも、一定時間以上静寂なら、isRecをディスアクティブに。\n",
    "                if self.silent > self.spaceleng or self.alive == False:\n",
    "                    #print(\"--->{}\".format(self.waddr-self.spaceleng, self.waddr))\n",
    "                    self.isVoice = 0\n",
    "                    self.standby = -1\n",
    "                    self.voiceOnWave[-1,1,0] = self.waddr - self.spaceleng + 3 #+3して発話後に若干余白をとる。\n",
    "        #print(\"detectVoice End\")\n",
    "            \n",
    "    def mellCepstrum(self, start, end=None):\n",
    "        #Mellの特微量を生成\n",
    "        if end != None:\n",
    "            mel_spec = ps.logfbank(self.wave[start:end].reshape(-1), samplerate=self.RATE, winlen=self.winlen, winstep=self.winstep, nfilt=self.nfilt, nfft=int(self.winlen*self.RATE))\n",
    "        else:\n",
    "            mel_spec = ps.logfbank(self.wave[start].reshape(-1), samplerate=self.RATE, winlen=self.winlen, winstep=self.winstep, nfilt=self.nfilt, nfft=int(self.winlen*self.RATE))\n",
    "        #deltaの特微量を生成\n",
    "        delta1= ps.delta(mel_spec, 4)\n",
    "        #delta-deltaの特微量を生成\n",
    "        delta2 = ps.delta(delta1, 4)\n",
    "        procd = np.empty((mel_spec.shape[0], self.nfilt, 3))\n",
    "        procd[:,:,0] = mel_spec#(mel_spec - self.mean1)/(self.std1+self.eps) #mel_spec#\n",
    "        procd[:,:,1] = delta1#(delta1 - self.mean2)/(self.std2+self.eps) #delta1#\n",
    "        procd[:,:,2] = delta2#(delta2 - self.mean3)/(self.std3+self.eps) #delta2#\n",
    "        return procd\n",
    "    \n",
    "    def mellshape(self, arrayelem=1):\n",
    "        datalen=self.CHUNK/self.RATE\n",
    "        shape=(datalen*arrayelem-self.winlen)/self.winstep+1 if arrayelem > 1 else 1\n",
    "        return math.ceil(shape)\n",
    "                \n",
    "    def extractMell(self):\n",
    "        #print(\"extractMell Start\")\n",
    "        #time.sleep(0.001)\n",
    "        #voiceOnWave上に新しい声が登録されたら\n",
    "        if self.voiceOnWave.shape[0] > self.pastVoiceOnWaveLeng:\n",
    "            #isVoiceがアクティブになった直後なら\n",
    "            if self.voiceOnWave[-1,0,0] + self.voiceleng == self.waddr: \n",
    "                self.lastwaddr = self.voiceOnWave[-1,0,0]\n",
    "                self.voiceOnWave[-1,0,1] = self.features.shape[0]\n",
    "                self.extractMellalive = True\n",
    "            #メル周波数ケプストラムの変換後のフレームが300フレーム以上になったら\n",
    "            if self.mellshape(self.waddr-self.lastwaddr) >= 300 and self.alive == True:\n",
    "                tmp = self.mellCepstrum(self.lastwaddr, self.waddr)\n",
    "                self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0) \n",
    "                self.features[-1] = tmp[0:300]\n",
    "                self.lastwaddr = self.waddr\n",
    "            #isVoiceがディアクティブになったら(レコーディングが終了したら)\n",
    "            elif self.voiceOnWave[-1,1,0] > 0 or self.alive == False:\n",
    "                #type1 この処理をした時だけ、必ずAngryとHappyが下がり、Sadが急上昇する。\n",
    "                #tmp = self.mellCepstrum(self.lastwaddr, self.waddr)\n",
    "                #self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0)\n",
    "                #self.features[-1,:-tmp.shape[0]] = self.features[-2,tmp.shape[0]:]\n",
    "                #self.features[-1,-tmp.shape[0]:] = tmp\n",
    "                #type2 この処理をした時だけ、必ずAngryとHappyが下がり、Sadが急上昇する。\n",
    "                tmp = self.mellCepstrum(self.lastwaddr, self.waddr)\n",
    "                self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0)\n",
    "                self.features[-1,:tmp.shape[0]] = tmp\n",
    "                #type3 この処理をした時だけ、必ずAngryとHappyが下がり、Sadが急上昇する。\n",
    "                #tmp = self.mellCepstrum(self.waddr-76, self.waddr)\n",
    "                #self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0)\n",
    "                #self.features[-1,:tmp.shape[0]] = tmp\n",
    "                self.voiceOnWave[-1,1,1] = self.features.shape[0]\n",
    "                self.pastVoiceOnWaveLeng = self.voiceOnWave.shape[0]\n",
    "                self.extractMellalive = False\n",
    "        #print(\"extractMell End\")\n",
    "    \n",
    "    def inverselabel(self, value):\n",
    "        tag = {'female_angry':0, 'female_disgust':1, 'female_fear':2, 'female_happy':3,\n",
    "     'female_neutral':4, 'female_sad':5, 'female_surprise':6, 'male_angry':7,\n",
    "     'male_disgust':8, 'male_fear':9, 'male_happy':10, 'male_neutral':11, 'male_sad':12,\n",
    "     'male_surprise':13}\n",
    "        return [k for k, v in tag.items() if v == value][0]\n",
    "    \n",
    "    def emotionRecognition(self):\n",
    "        #print(\"emotionRecognition Start\")\n",
    "        #time.sleep(0.001)\n",
    "        if self.faddr < self.features.shape[0]:\n",
    "            preds = self.loaded_model.predict_step(np.expand_dims(self.features[self.faddr],axis=0))\n",
    "            #print(preds.argmax(axis=1))\n",
    "            self.emotionResult = np.append(self.emotionResult, preds, 0)\n",
    "            print(\"{}\".format(self.inverselabel(self.emotionResult[-1].argmax())))\n",
    "            self.faddr +=1\n",
    "        #print(\"emotionRecognition End\")\n",
    "        \n",
    "    def terminate(self):\n",
    "        self.stream.stop_stream()\n",
    "        self.stream.close()\n",
    "    \n",
    "    def alpha(self):\n",
    "        #print(\"alpha start\")\n",
    "        ret = self.stream.read(self.CHUNK, exception_on_overflow = False)\n",
    "        self.stream.write(ret)\n",
    "        self.wave = np.append(self.wave, np.expand_dims(np.frombuffer(ret, dtype=\"int16\"), 0), axis=0)\n",
    "        #print(\"alpha end\")\n",
    "    \n",
    "    def bravo(self):\n",
    "        #print(\"bravo start\")\n",
    "        time.sleep(0.001)\n",
    "        #print(\"bravo end\")\n",
    "        \n",
    "    def charlie(self):\n",
    "        #print(\"charlie start\")\n",
    "        time.sleep(0.001)\n",
    "        #print(\"charlie end\")\n",
    "    \n",
    "    def delta(self):\n",
    "        #print(\"delta start\")\n",
    "        time.sleep(0.001)\n",
    "        #print(\"delta end\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            while self.alive:\n",
    "                executor.submit(fn=self.alpha)\n",
    "                executor.submit(fn=self.bravo)\n",
    "                executor.submit(fn=self.charlie)\n",
    "                executor.submit(fn=self.delta)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            executor.submit(fn=self.inputAudio())\n",
    "        \"\"\"\n",
    "        iA = Process(target=self.inputAudio())\n",
    "        iA.start()\n",
    "        while self.alive or self.extractMellalive:\n",
    "            print(\"T_T\")\n",
    "            self.detectVoice()\n",
    "            self.extractMell()\n",
    "            self.emotionRecognition()\n",
    "        self.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Realtime emotion recognition stoped.\n"
     ]
    }
   ],
   "source": [
    "eAn = emotionAnalizer()\n",
    "try:\n",
    "    eAn.run()\n",
    "except KeyboardInterrupt:\n",
    "    eAn.alive = False\n",
    "    print(\"Realtime emotion recognition stoped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存用コマンド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(\"output/\"+str(dt_now.isoformat()), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for no in eAn.voiceOnWave:\n",
    "    Y=eAn.wave[no[0,0]:no[1,0]].reshape(-1)\n",
    "    outd = struct.pack(\"h\" * len(Y), *Y)\n",
    "    filename = \"output/\"+str(dt_now.isoformat())+ \"/trimAt\" + str(no[0,0])+\"-\"+str(no[1,0]) + \".wav\"\n",
    "    # 書き出し\n",
    "    with wave.open(filename, 'w') as ww:\n",
    "        ww.setnchannels(1)\n",
    "        ww.setsampwidth(2)\n",
    "        ww.setframerate(44100)\n",
    "        ww.writeframes(outd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=eAn.wave.reshape(-1)\n",
    "outd = struct.pack(\"h\" * len(Y), *Y)\n",
    "filename = \"output/\"+str(dt_now.isoformat())+ \"/All.wav\"\n",
    "# 書き出し\n",
    "with wave.open(filename, 'w') as ww:\n",
    "    ww.setnchannels(1)\n",
    "    ww.setsampwidth(2)\n",
    "    ww.setframerate(44100)\n",
    "    ww.writeframes(outd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputAudio():\n",
    "        #print(\"inputAudio Start\")\n",
    "        alive = True\n",
    "        CHUNK=1024\n",
    "        RATE = 44100\n",
    "        wave = np.empty((0,1024),int)\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream=audio.open(format = pyaudio.paInt16,\n",
    "                    channels = 1,\n",
    "                    rate = RATE,\n",
    "                    frames_per_buffer = CHUNK,\n",
    "                    input = True,\n",
    "                    output=True)\n",
    "        while alive:\n",
    "            ret = stream.read(CHUNK, exception_on_overflow = False)\n",
    "            stream.write(ret)\n",
    "            wave = np.append(wave, np.expand_dims(np.frombuffer(ret, dtype=\"int16\"), 0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/naokitakatani/.pyenv/versions/3.7.9/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/naokitakatani/.pyenv/versions/3.7.9/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-41-a45729e88fac>\", line 13, in inputAudio\n",
      "    output=True)\n",
      "  File \"/Users/naokitakatani/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/pyaudio.py\", line 750, in open\n",
      "    stream = Stream(self, *args, **kwargs)\n",
      "  File \"/Users/naokitakatani/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/pyaudio.py\", line 441, in __init__\n",
      "    self._stream = pa.open(**arguments)\n",
      "OSError: [Errno -9986] Internal PortAudio error\n"
     ]
    }
   ],
   "source": [
    "iA = Process(target=inputAudio)\n",
    "iA.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "emotionResult = np.zeros((0,14))\n",
    "# loading json and model architecture \n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "with open('../2020_12_12_SERv3/extractMell.pkl', 'rb') as file:\n",
    "  features, emolabel = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deqpredict():\n",
    "    for index, data in enumerate(features):\n",
    "        print(index)\n",
    "        time.sleep(1)\n",
    "        preds = loaded_model.predict_step(np.expand_dims(data,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-5c4d8329e926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeqpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"elapsed_time:{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"[sec]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-732e6aa19935>\u001b[0m in \u001b[0;36mdeqpredict\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \"\"\"\n\u001b[1;32m    424\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 425\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2602\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2605\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2606\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dp = Process(target=deqpredict(), args=())\n",
    "dp.start()\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
