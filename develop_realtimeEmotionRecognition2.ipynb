{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling2D, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "import wave\n",
    "import struct\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import python_speech_features as ps\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import pyaudio\n",
    "import math\n",
    "import datetime\n",
    "from concurrent.futures.process import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import time\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotionAnalizer():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alive = True\n",
    "        self.extractMellalive = False\n",
    "        #inputAudioに必要な初期化\n",
    "        self.CHUNK=1024\n",
    "        self.RATE = 44100\n",
    "        self.wave = np.empty((0,1024),int)\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.stream=self.audio.open(format = pyaudio.paInt16,\n",
    "            channels = 1,\n",
    "            rate = self.RATE,\n",
    "            frames_per_buffer = self.CHUNK,\n",
    "            input = True,\n",
    "            output=True)\n",
    "        #detectAudioに必要な初期化\n",
    "        self.spaceleng = 10\n",
    "        self.voiceleng = 10\n",
    "        self.sokuonleng = 3\n",
    "        self.winlen = 0.08\n",
    "        self.winstep = 0.016\n",
    "        self.nfilt = 40 #周波数の分解能\n",
    "        self.waddr = 0\n",
    "        self.ave = np.empty((1,), float) #diffで過去のデータを参照するなので1つ余分に作っておく。\n",
    "        self.diff = np.empty((0,), float)\n",
    "        self.silent = 0\n",
    "        self.voice  = 0\n",
    "        self.standby = -1\n",
    "        self.isVoice = 0\n",
    "        self.voiceOnWave = np.empty((0,2,2), int)\n",
    "        #extractMellに必要な初期化\n",
    "        self.pastVoiceOnWaveLeng = 0\n",
    "        self.lastwaddr = 0\n",
    "        self.features = np.zeros((0, 300, self.nfilt, 3))\n",
    "        self.eps = 1e-5\n",
    "        with open('mean_and_std.pkl', 'rb') as file:\n",
    "          self.mean1,self.std1,self.mean2,self.std2,self.mean3,self.std3 = pickle.load(file)\n",
    "        #emotionRecognitionに必要な初期化\n",
    "        self.faddr = 0\n",
    "        self.emotionResult = np.zeros((0,14))\n",
    "        # loading json and model architecture \n",
    "        json_file = open('model_json.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into new model\n",
    "        self.loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "\n",
    "    def inputAudio(self):\n",
    "            #print(\"inputAudio Start\")\n",
    "            while self.alive:\n",
    "                ret = self.stream.read(self.CHUNK, exception_on_overflow = False)\n",
    "                self.stream.write(ret)\n",
    "                self.wave = np.append(self.wave, np.expand_dims(np.frombuffer(ret, dtype=\"int16\"), 0), axis=0)\n",
    "            #print(\"inputAudio End\")\n",
    "        \n",
    "    def detectVoice(self):\n",
    "        #print(\"detectVoice Start\")\n",
    "        #time.sleep(0.001)\n",
    "        if self.wave.shape[0] > 0:\n",
    "            self.waddr = self.wave.shape[0]-1 #wave変数の最新のデータは、その時のwaveの要素数から1引いたもの\n",
    "            self.ave = np.append(self.ave, np.mean(np.abs(self.wave[self.waddr])))\n",
    "            self.diff = np.append(self.diff, self.ave[-2]-self.ave[-1])\n",
    "            #声の部分か判定する。\n",
    "            if np.abs(self.diff[self.waddr]) > 100:\n",
    "                self.silent = 0\n",
    "                self.voice  = self.sokuonleng\n",
    "            else:\n",
    "                self.silent += 1\n",
    "                self.voice  -= 1 if self.voice > 0 else 0\n",
    "            #発話していないなら\n",
    "            if self.isVoice == 0:\n",
    "                if self.standby == -1:\n",
    "                    #スタンバイでない時、音を検知したら、スタンバイする(その時の時間を覚えておく)。\n",
    "                    if self.voice == self.sokuonleng:\n",
    "                        self.standby = self.waddr - 1 #発声する瞬間も大事な特微量なので、現在のアドレスより1コマ過去のものから判定開始\n",
    "                else:\n",
    "                    if self.waddr-self.standby >= self.voiceleng:\n",
    "                        #voiceleng以上発話していれば、isRecをアクティブに。\n",
    "                        if self.voice != 0:\n",
    "                            print(\"Record start{}\".format(self.standby, self.waddr))\n",
    "                            self.isVoice = 1\n",
    "                            self.voiceOnWave = np.append(self.voiceOnWave, np.zeros((1,2,2), int),0)\n",
    "                            self.voiceOnWave[-1,0,0] = self.standby\n",
    "                        #voicelengより短い発話だったら、スタンバイ解除\n",
    "                        elif self.voice == 0:\n",
    "                            self.standby = -1\n",
    "            #発話しているなら\n",
    "            else:\n",
    "                #もしも、一定時間以上静寂なら、isRecをディスアクティブに。\n",
    "                if self.silent > self.spaceleng or self.alive == False:\n",
    "                    #print(\"--->{}\".format(self.waddr-self.spaceleng, self.waddr))\n",
    "                    self.isVoice = 0\n",
    "                    self.standby = -1\n",
    "                    self.voiceOnWave[-1,1,0] = self.waddr - self.spaceleng + 3 #+3して発話後に若干余白をとる。\n",
    "        #print(\"detectVoice End\")\n",
    "            \n",
    "    def mellCepstrum(self, start, end=None):\n",
    "        #Mellの特微量を生成\n",
    "        if end != None:\n",
    "            mel_spec = ps.logfbank(self.wave[start:end].reshape(-1), samplerate=self.RATE, winlen=self.winlen, winstep=self.winstep, nfilt=self.nfilt, nfft=int(self.winlen*self.RATE))\n",
    "        else:\n",
    "            mel_spec = ps.logfbank(self.wave[start].reshape(-1), samplerate=self.RATE, winlen=self.winlen, winstep=self.winstep, nfilt=self.nfilt, nfft=int(self.winlen*self.RATE))\n",
    "        #deltaの特微量を生成\n",
    "        delta1= ps.delta(mel_spec, 4)\n",
    "        #delta-deltaの特微量を生成\n",
    "        delta2 = ps.delta(delta1, 4)\n",
    "        procd = np.empty((mel_spec.shape[0], self.nfilt, 3))\n",
    "        procd[:,:,0] = mel_spec#(mel_spec - self.mean1)/(self.std1+self.eps) #mel_spec#\n",
    "        procd[:,:,1] = delta1#(delta1 - self.mean2)/(self.std2+self.eps) #delta1#\n",
    "        procd[:,:,2] = delta2#(delta2 - self.mean3)/(self.std3+self.eps) #delta2#\n",
    "        return procd\n",
    "    \n",
    "    def mellshape(self, arrayelem=1):\n",
    "        datalen=self.CHUNK/self.RATE\n",
    "        shape=(datalen*arrayelem-self.winlen)/self.winstep+1 if arrayelem > 1 else 1\n",
    "        return math.ceil(shape)\n",
    "                \n",
    "    def extractMell(self):\n",
    "        #print(\"extractMell Start\")\n",
    "        #time.sleep(0.001)\n",
    "        #voiceOnWave上に新しい声が登録されたら\n",
    "        if self.voiceOnWave.shape[0] > self.pastVoiceOnWaveLeng:\n",
    "            #isVoiceがアクティブになった直後なら\n",
    "            if self.voiceOnWave[-1,0,0] + self.voiceleng == self.waddr: \n",
    "                self.lastwaddr = self.voiceOnWave[-1,0,0]\n",
    "                self.voiceOnWave[-1,0,1] = self.features.shape[0]\n",
    "                self.extractMellalive = True\n",
    "            #メル周波数ケプストラムの変換後のフレームが300フレーム以上になったら\n",
    "            if self.mellshape(self.waddr-self.lastwaddr) >= 300 and self.alive == True:\n",
    "                tmp = self.mellCepstrum(self.lastwaddr, self.waddr)\n",
    "                self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0) \n",
    "                self.features[-1] = tmp[0:300]\n",
    "                self.lastwaddr = self.waddr\n",
    "            #isVoiceがディアクティブになったら(レコーディングが終了したら)\n",
    "            elif self.voiceOnWave[-1,1,0] > 0 or self.alive == False:\n",
    "                #type1 この処理をした時だけ、必ずAngryとHappyが下がり、Sadが急上昇する。\n",
    "                #tmp = self.mellCepstrum(self.lastwaddr, self.waddr)\n",
    "                #self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0)\n",
    "                #self.features[-1,:-tmp.shape[0]] = self.features[-2,tmp.shape[0]:]\n",
    "                #self.features[-1,-tmp.shape[0]:] = tmp\n",
    "                #type2 この処理をした時だけ、必ずAngryとHappyが下がり、Sadが急上昇する。\n",
    "                tmp = self.mellCepstrum(self.lastwaddr, self.waddr)\n",
    "                self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0)\n",
    "                self.features[-1,:tmp.shape[0]] = tmp\n",
    "                #type3 この処理をした時だけ、必ずAngryとHappyが下がり、Sadが急上昇する。\n",
    "                #tmp = self.mellCepstrum(self.waddr-76, self.waddr)\n",
    "                #self.features = np.append(self.features, np.zeros((1, 300, self.nfilt, 3)),0)\n",
    "                #self.features[-1,:tmp.shape[0]] = tmp\n",
    "                self.voiceOnWave[-1,1,1] = self.features.shape[0]\n",
    "                self.pastVoiceOnWaveLeng = self.voiceOnWave.shape[0]\n",
    "                self.extractMellalive = False\n",
    "        #print(\"extractMell End\")\n",
    "    \n",
    "    def inverselabel(self, value):\n",
    "        tag = {'female_angry':0, 'female_disgust':1, 'female_fear':2, 'female_happy':3,\n",
    "     'female_neutral':4, 'female_sad':5, 'female_surprise':6, 'male_angry':7,\n",
    "     'male_disgust':8, 'male_fear':9, 'male_happy':10, 'male_neutral':11, 'male_sad':12,\n",
    "     'male_surprise':13}\n",
    "        return [k for k, v in tag.items() if v == value][0]\n",
    "    \n",
    "    def emotionRecognition(self):\n",
    "        #print(\"emotionRecognition Start\")\n",
    "        #time.sleep(0.001)\n",
    "        if self.faddr < self.features.shape[0]:\n",
    "            preds = self.loaded_model.predict_step(np.expand_dims(self.features[self.faddr],axis=0))\n",
    "            #print(preds.argmax(axis=1))\n",
    "            self.emotionResult = np.append(self.emotionResult, preds, 0)\n",
    "            print(\"{}\".format(self.inverselabel(self.emotionResult[-1].argmax())))\n",
    "            self.faddr +=1\n",
    "        #print(\"emotionRecognition End\")\n",
    "        \n",
    "    def terminate(self):\n",
    "        self.stream.stop_stream()\n",
    "        self.stream.close()\n",
    "    \n",
    "    def alpha(self):\n",
    "        #print(\"alpha start\")\n",
    "        ret = self.stream.read(self.CHUNK, exception_on_overflow = False)\n",
    "        self.stream.write(ret)\n",
    "        self.wave = np.append(self.wave, np.expand_dims(np.frombuffer(ret, dtype=\"int16\"), 0), axis=0)\n",
    "        #print(\"alpha end\")\n",
    "    \n",
    "    def bravo(self):\n",
    "        #print(\"bravo start\")\n",
    "        time.sleep(0.001)\n",
    "        #print(\"bravo end\")\n",
    "        \n",
    "    def charlie(self):\n",
    "        #print(\"charlie start\")\n",
    "        time.sleep(0.001)\n",
    "        #print(\"charlie end\")\n",
    "    \n",
    "    def delta(self):\n",
    "        #print(\"delta start\")\n",
    "        time.sleep(0.001)\n",
    "        #print(\"delta end\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            while self.alive:\n",
    "                executor.submit(fn=self.alpha)\n",
    "                executor.submit(fn=self.bravo)\n",
    "                executor.submit(fn=self.charlie)\n",
    "                executor.submit(fn=self.delta)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            executor.submit(fn=self.inputAudio())\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            while self.alive:\n",
    "                executor.submit(self.inputAudio)\n",
    "                executor.submit(self.detectVoice)\n",
    "                executor.submit(self.extractMell)\n",
    "                executor.submit(self.emotionRecognition)\n",
    "        \"\"\"\n",
    "        iA = Process(target=self.inputAudio())\n",
    "        iA.start()\n",
    "        while self.alive or self.extractMellalive:\n",
    "            print(\"T_T\")\n",
    "            self.detectVoice()\n",
    "            self.extractMell()\n",
    "            self.emotionRecognition()\n",
    "        \"\"\"\n",
    "        self.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Realtime emotion recognition stoped.\n"
     ]
    }
   ],
   "source": [
    "eAn = emotionAnalizer()\n",
    "try:\n",
    "    eAn.run()\n",
    "except KeyboardInterrupt:\n",
    "    eAn.alive = False\n",
    "    print(\"Realtime emotion recognition stoped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存用コマンド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(\"output/\"+str(dt_now.isoformat()), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for no in eAn.voiceOnWave:\n",
    "    Y=eAn.wave[no[0,0]:no[1,0]].reshape(-1)\n",
    "    outd = struct.pack(\"h\" * len(Y), *Y)\n",
    "    filename = \"output/\"+str(dt_now.isoformat())+ \"/trimAt\" + str(no[0,0])+\"-\"+str(no[1,0]) + \".wav\"\n",
    "    # 書き出し\n",
    "    with wave.open(filename, 'w') as ww:\n",
    "        ww.setnchannels(1)\n",
    "        ww.setsampwidth(2)\n",
    "        ww.setframerate(44100)\n",
    "        ww.writeframes(outd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=eAn.wave.reshape(-1)\n",
    "outd = struct.pack(\"h\" * len(Y), *Y)\n",
    "filename = \"output/\"+str(dt_now.isoformat())+ \"/All.wav\"\n",
    "# 書き出し\n",
    "with wave.open(filename, 'w') as ww:\n",
    "    ww.setnchannels(1)\n",
    "    ww.setsampwidth(2)\n",
    "    ww.setframerate(44100)\n",
    "    ww.writeframes(outd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputAudio():\n",
    "        #print(\"inputAudio Start\")\n",
    "        alive = True\n",
    "        CHUNK=1024\n",
    "        RATE = 44100\n",
    "        wave = np.empty((0,1024),int)\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream=audio.open(format = pyaudio.paInt16,\n",
    "                    channels = 1,\n",
    "                    rate = RATE,\n",
    "                    frames_per_buffer = CHUNK,\n",
    "                    input = True,\n",
    "                    output=True)\n",
    "        while alive:\n",
    "            ret = stream.read(CHUNK, exception_on_overflow = False)\n",
    "            stream.write(ret)\n",
    "            wave = np.append(wave, np.expand_dims(np.frombuffer(ret, dtype=\"int16\"), 0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/naokitakatani/.pyenv/versions/3.7.9/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/naokitakatani/.pyenv/versions/3.7.9/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-41-a45729e88fac>\", line 13, in inputAudio\n",
      "    output=True)\n",
      "  File \"/Users/naokitakatani/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/pyaudio.py\", line 750, in open\n",
      "    stream = Stream(self, *args, **kwargs)\n",
      "  File \"/Users/naokitakatani/Documents/2020_12_22_realtime_SERv3/env/lib/python3.7/site-packages/pyaudio.py\", line 441, in __init__\n",
      "    self._stream = pa.open(**arguments)\n",
      "OSError: [Errno -9986] Internal PortAudio error\n"
     ]
    }
   ],
   "source": [
    "iA = Process(target=inputAudio)\n",
    "iA.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "emotionResult = np.zeros((0,14))\n",
    "# loading json and model architecture \n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "with open('extractMell_part.pkl', 'rb') as file:\n",
    "  features, emolabel = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deqpredict():\n",
    "    for index, data in enumerate(features):\n",
    "        print(index)\n",
    "        time.sleep(1)\n",
    "        preds = loaded_model.predict_step(np.expand_dims(data,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5c4d8329e926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeqpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"elapsed_time:{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"[sec]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-732e6aa19935>\u001b[0m in \u001b[0;36mdeqpredict\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dp = Process(target=deqpredict(), args=())\n",
    "dp.start()\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHello():\n",
    "    while True:\n",
    "        print(\"Hello world!\")\n",
    "        time.sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d55dc887bd87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeqpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprintHello\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-732e6aa19935>\u001b[0m in \u001b[0;36mdeqpredict\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.submit(deqpredict)\n",
    "    executor.submit(printHello)\n",
    "    elapsed_time = time.time() - start\n",
    "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
